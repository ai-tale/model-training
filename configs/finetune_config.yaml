# Model configuration for fairy tale generation fine-tuning

# Basic settings
experiment_name: "storyteller-v1"
seed: 42
output_dir: "models/storyteller-v1"
overwrite_output_dir: true

# Model parameters
base_model: "gpt2-medium"  # Options: gpt2, gpt2-medium, gpt2-large, t5-base, etc.
model_type: "causal_lm"    # Options: causal_lm, seq2seq_lm
tokenizer_name: null       # If null, use the same as base_model
max_length: 1024
use_fast_tokenizer: true

# Training parameters
training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 5.0e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  lr_scheduler_type: "linear"  # Options: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup
  warmup_ratio: 0.1
  warmup_steps: 0
  logging_steps: 100
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 3
  fp16: true
  fp16_opt_level: "O1"  # Options: O1, O2, O3

# Dataset parameters
dataset:
  train_file: "data/processed/fairy_tales_train.json"
  validation_file: "data/processed/fairy_tales_val.json"
  text_column: "text"
  line_by_line: false
  max_train_samples: null  # Set a value to limit the number of training samples
  max_eval_samples: null   # Set a value to limit the number of evaluation samples
  preprocessing_num_workers: 4
  overwrite_cache: false
  
# Generation parameters for evaluation
generation:
  max_length: 512
  min_length: 50
  do_sample: true
  top_k: 50
  top_p: 0.95
  temperature: 0.8
  num_beams: 1
  no_repeat_ngram_size: 3
  
# Logging and evaluation
evaluation:
  evaluate_during_training: true
  prediction_loss_only: false

# Logging
logging:
  use_wandb: true
  wandb_project: "ai-tale"
  wandb_entity: "dreamerai" 